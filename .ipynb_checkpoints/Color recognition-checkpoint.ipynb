{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from  torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading  the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor() ])\n",
    "train = ImageFolder(root=r'C:\\Users\\jeremiah\\Desktop\\colors\\train', transform = compose)\n",
    "test = ImageFolder(root=r'C:\\Users\\jeremiah\\Desktop\\colors\\test', transform = compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train ,shuffle = True, batch_size = 50) \n",
    "test_loader = DataLoader(test ,shuffle = True, batch_size = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "literal_label = os.listdir(r'C:\\Users\\jeremiah\\Desktop\\colors\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAABgCAYAAABhXE4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAFMklEQVR4nO2cXYhUZRjHf/9ddyMxLbNkbU3LJNwrK+mDLiKsKG/MiyKDVCq2KKOiLiS68CKiG4MiCDba0Ej7sCAvjApTYiFCCfvQpdLM1LbNIPtwyVp7ujhnc1pndHbPs2fOyecHw5w55513nvnN+54z7+F9H5kZgQ9NjQ7g/0TIdCRkOhIyHQmZjoRMR0ojU9I7kpbWUW6mJJM0Lo+4KimNTDO7ycxWe9YpaZmkHq/6SiOzFJhZYR7ACmD9sH3PAM8CW4C7031NwOPAXuBHYA0wKT02EzBgXPp6EvAi0AccAJ4AmoE5wB/AUeB34FDW+IvWMtcBCyRNBJDUDNwKrB1Wbln6uBa4EJgAPFejztXAIHARcAlwA8mP0gvcC3xkZhPM7MzM0Te6NVZpnT3AknT7emB3ur2FYy1zE3BfxXsuBv4CxlHRMoGpwBHg9Iqyi4HN6fYyoMcr9tyveHWwluQLrwFu5/hWCTCNpIsPsZdj8iqZAbQAfZKG9jUB+xzj/ZciynwDWCWpHVgEXFWlzPckooY4n6Qr9wPtFfv3kbTMKWY2WKUe11tmRTtnYmYHSbr0S8Ce9Nw2nHXAw5IukDQBeBJ4bbgwM+sD3iP5cSZKapI0S9I1aZF+oF1Sq0fshZOZsha4jupdHKAbeBn4ENhDclV+oEbZJUArsBP4GVgPtKXHPgB2AD9I+ilr0LK4OexGUVtmKQmZjmSSKelGSV9K2iVphVdQZWXU58x0dPIVyR/r/cBWYLGZ7fQLr1xkaZmXA7vM7Bsz+xN4FVjoE1Y5yfKn/Tz+O5LYD1wxvJCkTqAToKW55bKzJ07J8JGN55fDhxg4MqBqx7LIrFbhcecMM+sCugDaJk+zO+d3ZvjIxtO9qavmsSzdfD8wveJ1O8kw75Qli8ytwOx0SNcK3AZs8AmrnIy6m5vZoKTlwLskN1u7zWyHW2QlJNNdIzPbCGx0iqX0xAjIkZDpSMh0JGQ6EjIdCZmOhExHQqYjIdORkOlIyHQkZDoSMh0JmY6ETEdCpiMh05GQ6UjIdCRkOhIyHQmZjpxUpqTpkjZL6pW0Q9KD6f7Jkt6X9HX6fNbYh1ts6mmZg8AjZjYHuBK4X1IHyWqyTWY2m2Rdzik/P/OkMs2sz8w+Sbd/A3pJZsAtJFn9Rfp881gFWRZGdM6UNJNkydzHwNR0acjQEpFza7ynU9I2SdsGjgxki7bg1C0zXW/zJvCQmf1a7/vMrMvM5pnZvPGnjR9NjKWhLpmSWkhEvmJmb6W7+yW1pcfbSFbXntLUczUXyRLjXjN7uuLQBmAoM8FS4G3/8MpFPbPgrgbuAD6XtD3d9xjwFPC6pLuA74BbxibE8nBSmWbWQ/Up1wDzfcMpNzECciRkOhIyHQmZjoRMR0KmIyHTkZDpSMh0JGQ6EjIdCZmO5JqKR9JB4DCQOYfQGDCF+uKaYWbnVDuQe14jSdvMbF6uH1oHHnFFN3ckZDrSCJm1k1w0lsxxRS44R6KbO5KbzKKkOjvB3KmVkg5I2p4+Foy48pzyCDcDu0mSLbcCnwIdDcpp3AZcmm6fQZKCrQNYCTyape68WmZhUp2dYO5UZvKSWS3VmcsXyMKwuVMAyyV9Jql7NFMk85JZV6qzPKkyd+p5YBYwlyRB/qqR1pmXzEKlOqs2d8rM+s3sqJn9DbxAcmoaEXnJLEyqs1pzp4YmoaUsAr4Yad255GkvWKqzWnOnFkuaS3L6+Ra4Z6QVxwjIkRgBORIyHQmZjoRMR0KmIyHTkZDpSMh05B9jSq2CQzC5pgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "for i in range(3): \n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(img_data[i].permute(1,2,0))\n",
    "    plt.title(literal_label[label[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        self.relu = nn.ReLU(), #output 16 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "        self.pool = nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "        self.fc1 = nn.Linear(128*8*8, 1024),\n",
    "        self.fc2 = nn.ReLU(),\n",
    "        self.fc2 = nn.Linear(1024, 512),\n",
    "        self.fc2 = nn.ReLU(),\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 32, 3, 3], expected input[50, 16, 32, 32] to have 32 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3b80d1737816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-037af275df29>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xb)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 444\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 32, 3, 3], expected input[50, 16, 32, 32] to have 32 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "for i in range(epoch): \n",
    "    for idx, (img, label) in enumerate(train_loader):\n",
    "        output = model(img)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
